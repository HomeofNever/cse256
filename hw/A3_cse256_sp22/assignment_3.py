# -*- coding: utf-8 -*-
"""Assignment 3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10l6avn9whGKAbHTMKKKnSt1MljCYOSM2
"""

# Commented out IPython magic to ensure Python compatibility.
#!/bin/python
# !pip install tabulate

from __future__ import division
from __future__ import print_function
from __future__ import absolute_import

import numpy as np
import collections
from math import log
import sys

import matplotlib.pyplot as plt
# %matplotlib inline

class LangModel:
    def fit_corpus(self, corpus):
        """Learn the language model for the whole corpus.

        The corpus consists of a list of sentences."""
        for s in corpus:
            self.fit_sentence(s)
        self.norm()

    def perplexity(self, corpus):
        """Computes the perplexity of the corpus by the model.

        Assumes the model uses an EOS symbol at the end of each sentence.
        """
        return pow(2.0, self.entropy(corpus))

    def entropy(self, corpus):
        num_words = 0.0
        sum_logprob = 0.0
        for s in corpus:
            num_words += len(s) + 1 # for EOS
            sum_logprob += self.logprob_sentence(s)
        return -(1.0/num_words)*(sum_logprob)

    def logprob_sentence(self, sentence):
        p = 0.0
        for i in range(len(sentence)):
            p += self.cond_logprob(sentence[i], sentence[:i])
        p += self.cond_logprob('END_OF_SENTENCE', sentence)
        return p

    # required, update the model when a sentence is observed
    def fit_sentence(self, sentence): pass
    # optional, if there are any post-training steps (such as normalizing probabilities)
    def norm(self): pass
    # required, return the log2 of the conditional prob of word, given previous words
    def cond_logprob(self, word, previous): pass
    # required, the list of words the language model suports (including EOS)
    def vocab(self): pass

class Unigram(LangModel):
    def __init__(self, backoff = 0.000001):
        self.model = dict()
        self.lbackoff = log(backoff, 2)
        self.backoff_count = 0

    def inc_word(self, w):
        if w in self.model:
            self.model[w] += 1.0
        else:
            self.model[w] = 1.0

    def fit_sentence(self, sentence):
        for w in sentence:
            self.inc_word(w)
        self.inc_word('END_OF_SENTENCE')

    def norm(self):
        """Normalize and convert to log2-probs."""
        tot = 0.0
        for word in self.model:
            tot += self.model[word]
        ltot = log(tot, 2)
        for word in self.model:
            self.model[word] = log(self.model[word], 2) - ltot

    def cond_logprob(self, word, previous):
        if word in self.model:
            return self.model[word]
        else:
            self.backoff_count += 1
            return self.lbackoff

    def vocab(self):
        return self.model.keys()

#!/bin/python
##### Starter Code

from __future__ import print_function

import random
from math import log
import numpy as np

class Sampler:

    def __init__(self, lm, temp = 1.0):
        """Sampler for a given language model.

        Supports the use of temperature, i.e. how peaky we want to treat the
        distribution as. Temperature of 1 means no change, temperature <1 means
        less randomness (samples high probability words even more), and temp>1
        means more randomness (samples low prob words more than otherwise). See
        simulated annealing for what this means.
        """
        self.lm = lm
        self.rnd = random.Random()
        self.temp = temp

    def sample_sentence(self, prefix = [], max_length = 20):
        """Sample a random sentence (list of words) from the language model.

        Samples words till either EOS symbol is sampled or max_length is reached.
        Does not make any assumptions about the length of the context.
        """
        i = 0
        sent = prefix
        word = self.sample_next(sent, False)
        while i <= max_length and word != "END_OF_SENTENCE":
            sent.append(word)
            word = self.sample_next(sent)
            i += 1
        return sent

    def sample_next(self, prev, incl_eos = True):
        """Samples a single word from context.

        Can be useful to debug the model, for example if you have a bigram model,
        and know the probability of X-Y should be really high, you can run
        sample_next([Y]) to see how often X get generated.

        incl_eos determines whether the space of words should include EOS or not.
        """
        wps = []
        tot = -np.inf # this is the log (total mass)
        for w in self.lm.vocab():
            if not incl_eos and w == "END_OF_SENTENCE":
                continue
            lp = self.lm.cond_logprob(w, prev)
            wps.append([w, lp/self.temp])
            tot = np.logaddexp2(lp/self.temp, tot)
        p = self.rnd.random()
        word = self.rnd.choice(wps)[0]
        s = -np.inf # running mass
        for w,lp in wps:
            s = np.logaddexp2(s, lp)
            if p < pow(2, s-tot):
                word = w
                break
        return word

def main_generator():
    unigram = Unigram()
    corpus = [
        [ "sam", "i", "am" ]
    ]
    unigram.fit_corpus(corpus)
    print(unigram.model)
    sampler = Sampler(unigram)
    for i in range(10):
        print(i, ":", " ".join(str(x) for x in sampler.sample_sentence([])))

main_generator()

# Do any data prep, unzip
def init():
  from google.colab import drive
  drive.mount('/content/drive')

# Return base dir from the homework
def base_dir():
  return "/content/drive/MyDrive/Colab Notebooks/CSE256/A3_cse256_sp22/"


def textToTokens(text):
    """Converts input string to a corpus of tokenized sentences.

    Assumes that the sentences are divided by newlines (but will ignore empty sentences).
    You can use this to try out your own datasets, but is not needed for reading the homework data.
    """
    corpus = []
    sents = text.split("\n")
    from sklearn.feature_extraction.text import CountVectorizer
    count_vect = CountVectorizer()
    count_vect.fit(sents)
    tokenizer = count_vect.build_tokenizer()
    for s in sents:
        toks = tokenizer(s)
        if len(toks) > 0:
            corpus.append(toks)
    return corpus

def file_splitter(filename, seed = 0, train_prop = 0.7, dev_prop = 0.15,
    test_prop = 0.15):
    """Splits the lines of a file into 3 output files."""
    import random
    rnd = random.Random(seed)
    basename = filename[:-4]
    train_file = open(basename + ".train.txt", "w")
    test_file = open(basename + ".test.txt", "w")
    dev_file = open(basename + ".dev.txt", "w")
    with open(filename, 'r') as f:
        for l in f.readlines():
            p = rnd.random()
            if p < train_prop:
                train_file.write(l)
            elif p < train_prop + dev_prop:
                dev_file.write(l)
            else:
                test_file.write(l)
    train_file.close()
    test_file.close()
    dev_file.close()

def read_texts(tarfname, dname):
    """Read the data from the homework data file.

    Given the location of the data archive file and the name of the
    dataset (one of brown, reuters, or gutenberg), this returns a
    data object containing train, test, and dev data. Each is a list
    of sentences, where each sentence is a sequence of tokens.
    """
    import tarfile
    tar = tarfile.open(tarfname, "r:gz", errors = 'replace')
    for member in tar.getmembers():
        if dname in member.name and ('train.txt') in member.name:
            print('\ttrain: %s'%(member.name))
            train_txt = str(tar.extractfile(member).read(), errors='replace')
        elif dname in member.name and ('test.txt') in member.name:
            print('\ttest: %s'%(member.name))
            test_txt = str(tar.extractfile(member).read(), errors='replace')
        elif dname in member.name and ('dev.txt') in member.name:
            print('\tdev: %s'%(member.name))
            dev_txt = str(tar.extractfile(member).read(), errors='replace')

    from sklearn.feature_extraction.text import CountVectorizer
    count_vect = CountVectorizer()
    count_vect.fit(train_txt.split("\n"))
    tokenizer = count_vect.build_tokenizer()
    class Data: pass
    data = Data()
    data.train = []
    for s in train_txt.split("\n"):
        toks = tokenizer(s)
        if len(toks) > 0:
            data.train.append(toks)
    data.test = []
    for s in test_txt.split("\n"):
        toks = tokenizer(s)
        if len(toks) > 0:
            data.test.append(toks)
    data.dev = []
    for s in dev_txt.split("\n"):
        toks = tokenizer(s)
        if len(toks) > 0:
            data.dev.append(toks)
    print(dname," read.", "train:", len(data.train), "dev:", len(data.dev), "test:", len(data.test))
    return data

def learn_unigram(data, verbose=True):
    """Learns a unigram model from data.train.

    It also evaluates the model on data.dev and data.test, along with generating
    some sample sentences from the model.
    """
    unigram = Unigram()
    unigram.fit_corpus(data.train)
    if verbose:
        print("vocab:", len(unigram.vocab()))
        # evaluate on train, test, and dev
        unigram.backoff_count = 0
        print("train:", unigram.perplexity(data.train))
        print("dev  :", unigram.perplexity(data.dev))
        print("test :", unigram.perplexity(data.test))
        btc = unigram.backoff_count
        print(" backoff : ", unigram.backoff_count)
        sampler = Sampler(unigram)
        print("sample 1: ", " ".join(str(x) for x in sampler.sample_sentence([])))
        print("sample 2: ", " ".join(str(x) for x in sampler.sample_sentence([])))
    return unigram, btc

def print_table(table, row_names, col_names, latex_file = None):
    """Pretty prints the table given the table, and row and col names.

    If a latex_file is provided (and tabulate is installed), it also writes a
    file containing the LaTeX source of the table (which you can \input into your report)
    """
    try:
        from tabulate import tabulate
        rfmt = "{:>15} "
        row_format = rfmt * (len(col_names) + 1)
        rows = map(lambda rt: [rt[0]] + rt[1], zip(row_names,table.tolist()))

        print(tabulate(rows, headers = [""] + col_names))
        if latex_file is not None:
            latex_str = tabulate(rows, headers = [""] + col_names, tablefmt="latex")
            with open(latex_file, 'w') as f:
                f.write(latex_str)
                f.close()
    except ImportError as e:
        for row_name, row in zip(row_names, table):
            print(row_format.format(row_name, *row))

##### Unigram Model

dnames = ["brown", "reuters", "gutenberg"]
datas = []
models = []
unigram_btc = []
# Learn the models for each of the domains, and evaluate it
for dname in dnames:
    print("-----------------------")
    print(dname)
    data = read_texts(base_dir() + "data/corpora.tar.gz", dname)
    datas.append(data)
    model, btc = learn_unigram(data)
    models.append(model)
    unigram_btc.append(btc)
# compute the perplexity of all pairs
n = len(dnames)
perp_dev = np.zeros((n,n))
perp_test = np.zeros((n,n))
perp_train = np.zeros((n,n))
for i in range(n):
    for j in range(n):
        perp_dev[i][j] = models[i].perplexity(datas[j].dev)
        perp_test[i][j] = models[i].perplexity(datas[j].test)
        perp_train[i][j] = models[i].perplexity(datas[j].train)

print("-------------------------------")
print("x train")
print_table(perp_train, dnames, dnames, "table-train.tex")
print("-------------------------------")
print("x dev")
print_table(perp_dev, dnames, dnames, "table-dev.tex")
print("-------------------------------")
print("x test")
print_table(perp_test, dnames, dnames, "table-test.tex")

##### Question 2.1
# Varies train size
# creating the dataset
# set width of bar
barWidth = 0.25
fig = plt.subplots(figsize =(12, 8))
 
# set height of bar
brown_plt = [perp_train[0][0], perp_dev[0][0], perp_test[0][0]]
reuters_plt = [perp_train[1][1], perp_dev[1][1], perp_test[1][1]]
guten_plt = [perp_train[2][2], perp_dev[2][2], perp_test[2][2]]

# Set position of bar on X axis
br1 = np.arange(len(brown_plt))
br2 = [x + barWidth for x in br1]
br3 = [x + barWidth for x in br2]
 
# Make the plot
plt.bar(br1, brown_plt, color ='r', width = barWidth,
        edgecolor ='grey', label ='Brown')
plt.bar(br2, reuters_plt, color ='g', width = barWidth,
        edgecolor ='grey', label ='Reuters')
plt.bar(br3, guten_plt, color ='b', width = barWidth,
        edgecolor ='grey', label ='Gutenberg')
 
# Adding Xticks
plt.xlabel('Set', fontweight ='bold', fontsize = 15)
plt.ylabel('Perplexity', fontweight ='bold', fontsize = 15)
plt.xticks([r + barWidth for r in range(len(brown_plt))],
        ["train", "dev", "test"])
 
plt.legend()
plt.show()

##### Trigram Model

from typing import DefaultDict

START = '<START>'
EOS = 'END_OF_SENTENCE'

class TrigramBackOff(LangModel):
    def __init__(self, delta = 0, backoff= 0.000001, lamb = [0.55, 0.35, 0.1]):
        self.model = DefaultDict(int)
        self.vocabulary = DefaultDict(int)
        self.context = DefaultDict(int)
        
        self.bi_model = DefaultDict(int)
        self.bi_context = DefaultDict(int)

        self.uni = DefaultDict(int)

        self.delta = delta
        self.lamb = lamb
        self.backoff = log(backoff, 2)
        
    def fit_sentence(self, sentence):
        s = [START, START] + sentence + [EOS]
        l = len(s)
        for i in range(2, l):
          self.vocabulary[s[i]] += 1
          self.model[(s[i - 2], s[i - 1], s[i])] += 1
          self.context[(s[i - 2], s[i - 1])] += 1
        
        s = [START] + sentence + [EOS]
        l = len(s)
        for i in range(1, l):
          self.bi_model[(s[i - 1], s[i])] += 1
          self.bi_context[s[i - 1]] += 1

        for i in sentence:
          self.uni[i] += 1

    def norm(self):
        v = len(self.vocab()) * self.delta

        for word in self.model:
            self.model[word] = (log(self.model[word] + self.delta, 2) - log(self.context[word[:2]] + v)) * self.lamb[0]

        for word in self.bi_model:
            self.bi_model[word] = (log(self.bi_model[word] + self.delta, 2) - log(self.bi_context[word[0]] + v, 2)) * self.lamb[1]

        tot = sum(self.uni.values())

        for word in self.uni:
            self.uni[word] = (log(self.uni[word] + self.delta, 2) - log(tot + v, 2)) * self.lamb[2]

        # update backoff
        if self.delta != 0:
          self.backoff = log(self.delta, 2) - log(v,2)

    def cond_logprob(self, word, previous):
        btm = (EOS, EOS)
        if len(previous) >= 2:
          btm = (previous[-2], previous[-1])
        elif len(previous) > 0:
          btm = (START, previous[-1])

        top = btm + (word,)

        if top in self.model and btm in self.bi_model and word in self.uni:
            return self.model[top] + self.bi_model[btm] + self.uni[word]
        else:
            return self.backoff

    def vocab(self):
        return self.vocabulary.keys()


class Trigram(LangModel):
    def __init__(self, delta = 1):
        self.model = DefaultDict(int)
        self.vocabulary = DefaultDict(int)
        self.context = DefaultDict(int)
        self.delta = delta
        self.backoff_count = 0
        
    def fit_sentence(self, sentence):
        s = [START, START] + sentence + [EOS]
        l = len(s)
        self.vocabulary[START] += 2
        for i in range(2, l):
          self.vocabulary[s[i]] += 1
          self.model[(s[i - 2], s[i - 1], s[i])] += 1
          self.context[(s[i - 2], s[i - 1])] += 1

    def norm(self):
        v = len(self.vocab()) * self.delta

        for word in self.model:
            self.model[word] = (log(self.model[word] + self.delta, 2) - log(self.context[word[:2]] + v, 2))

        # update backoff
        self.backoff = log(self.delta, 2) - log(v,2)

        # Freq word
        ss = 0
        for i in self.vocabulary.values():
          if i > 4:
            ss += 1
        
        print("Freq > 4: {}, percent: {}".format(ss, ss / len(self.vocabulary)))



    def cond_logprob(self, word, previous):
        top = (START, START, word)
        if len(previous) > 1:
          top = (previous[-2], previous[-1], word)
        elif len(previous) > 0:
          top = (START, previous[-1], word)

        if top in self.model:
            return self.model[top]
        else:
            self.backoff_count += 1
            return self.backoff

    def vocab(self):
        return self.vocabulary.keys()

##### Test Trigram

def tri_generator():
    trigram = Trigram()
    corpus = [
        [ "sam", "i", "am" ]
    ]
    trigram.fit_corpus(corpus)
    print(trigram.model)
    sampler = Sampler(trigram)
    for i in range(10):
        print(i, ":", " ".join(str(x) for x in sampler.sample_sentence([])))

tri_generator()

##### Train Trigram

def learn_trigram(data, verbose=True):
    """Learns a unigram model from data.train.

    It also evaluates the model on data.dev and data.test, along with generating
    some sample sentences from the model.
    """
    trigram = Trigram(delta=0.01)
    trigram.fit_corpus(data.train)
    if verbose:
        print("vocab:", len(trigram.vocab()))
        # evaluate on train, test, and dev
        trigram.backoff_count = 0
        print("train:", trigram.perplexity(data.train))
        print("dev  :", trigram.perplexity(data.dev))
        print("test :", trigram.perplexity(data.test))
        print("  backoff :", trigram.backoff_count)
        btc = trigram.backoff_count
        sampler = Sampler(trigram)
        print("sample 1: ", " ".join(str(x) for x in sampler.sample_sentence([])))
        print("sample 2: ", " ".join(str(x) for x in sampler.sample_sentence([])))
    return trigram, btc

tri_models = []
tri_models_btc = []
for dname in dnames:
    print("-----------------------")
    print(dname)
    data = read_texts(base_dir() + "data/corpora.tar.gz", dname)
    datas.append(data)
    model, btc = learn_trigram(data)
    tri_models.append(model)
    tri_models_btc.append(btc)
# compute the perplexity of all pairs
n = len(dnames)
tri_perp_dev = np.zeros((n,n))
tri_perp_test = np.zeros((n,n))
tri_perp_train = np.zeros((n,n))
for i in range(n):
    for j in range(n):
        tri_perp_dev[i][j] = tri_models[i].perplexity(datas[j].dev)
        tri_perp_test[i][j] = tri_models[i].perplexity(datas[j].test)
        tri_perp_train[i][j] = tri_models[i].perplexity(datas[j].train)

print("-------------------------------")
print("x train")
print_table(tri_perp_train, dnames, dnames, "table-train.tex")
print("-------------------------------")
print("x dev")
print_table(tri_perp_dev, dnames, dnames, "table-dev.tex")
print("-------------------------------")
print("x test")
print_table(tri_perp_test, dnames, dnames, "table-test.tex")

##### Question 3 Graph 1
# Varies train size
# creating the dataset
# set width of bar
barWidth = 0.25
fig = plt.subplots(figsize =(12, 8))
 
# set height of bar
brown_plt = [tri_perp_train[0][0], tri_perp_dev[0][0], tri_perp_test[0][0]]
reuters_plt = [tri_perp_train[1][1], tri_perp_dev[1][1], tri_perp_test[1][1]]
guten_plt = [tri_perp_train[2][2], tri_perp_dev[2][2], tri_perp_test[2][2]]

# Set position of bar on X axis
br1 = np.arange(len(brown_plt))
br2 = [x + barWidth for x in br1]
br3 = [x + barWidth for x in br2]
 
# Make the plot
plt.bar(br1, brown_plt, color ='r', width = barWidth,
        edgecolor ='grey', label ='Brown')
plt.bar(br2, reuters_plt, color ='g', width = barWidth,
        edgecolor ='grey', label ='Reuters')
plt.bar(br3, guten_plt, color ='b', width = barWidth,
        edgecolor ='grey', label ='Gutenberg')
 
# Adding Xticks
plt.xlabel('Set', fontweight ='bold', fontsize = 15)
plt.ylabel('Perplexity', fontweight ='bold', fontsize = 15)
plt.xticks([r + barWidth for r in range(len(brown_plt))],
        ["train", "dev", "test"])
 
plt.legend()
plt.show()

##### Question 3 Graph 2
# Varies train size
# creating the dataset
# set width of bar
barWidth = 0.25
fig = plt.subplots(figsize =(12, 8))
 
# set height of bar

# Set position of bar on X axis
br1 = np.arange(len(unigram_btc))
br2 = [x + barWidth for x in br1]
br3 = [x + barWidth for x in br2]
 
# Make the plot
plt.bar(br1, unigram_btc, color ='r', width = barWidth,
        edgecolor ='grey', label ='Unigram')
plt.bar(br2, tri_models_btc, color ='g', width = barWidth,
        edgecolor ='grey', label ='Trigram')
    
# Adding Xticks
plt.xlabel('Set', fontweight ='bold', fontsize = 15)
plt.ylabel('BackOff Count During Dev&Test', fontweight ='bold', fontsize = 15)
plt.xticks([r + barWidth for r in range(len(brown_plt))],
        dnames)
 
plt.legend()
plt.show()

##### Question 3 Sample
sentences = [
    "To be or not to be",
    "Make America great again",
    "Tesla posted a record 3.3 billion profit in the first three months of 2022 with sales of its vehicles up 81% from last year"
]

test_dev= np.zeros((n,n))
test_preps = np.zeros((n, n))
for i, s in enumerate(sentences):
  ls = []
  for j, t in enumerate(tri_models):
    test_dev[i][j] = t.logprob_sentence(s.split(" "))
    test_preps[i][j] = t.perplexity([s.split(" ")])

print_table(test_dev, sentences, dnames)
print_table(test_preps, sentences, dnames)

print("-----------------------")
print("brown and reuters")
dname = "brown"
brown_data = read_texts(base_dir() + "data/corpora.tar.gz", dname)
reuters_data = read_texts(base_dir() + "data/corpora.tar.gz", "reuters")

# Mix Data
brown_data.train += random.sample(reuters_data.train, int(len(reuters_data.train) * 0.05))
brown_data.dev += random.sample(reuters_data.dev, int(len(reuters_data.dev) * 0.05))

mix_model, _ = learn_trigram(brown_data)
five_percent = mix_model.perplexity(reuters_data.test)

##### Adoptions

print("-----------------------")
print("brown and reuters full set")
dname = "brown"
brown_data = read_texts(base_dir() + "data/corpora.tar.gz", dname)
reuters_data = read_texts(base_dir() + "data/corpora.tar.gz", "reuters")

# Mix Data
brown_data.train += reuters_data.train
brown_data.dev += reuters_data.dev

full_mix_model, _ = learn_trigram(brown_data)
full_mix_p = full_mix_model.perplexity(reuters_data.test)

mixed_name = ["Baseline", "5% mixed", "Full mixed", "Reuter"]
mixed_perplexity = [tri_perp_test[0][1], five_percent, full_mix_p, tri_perp_test[1][1]]
 
# Figure Size
fig = plt.figure(figsize =(10, 7))
 
# Horizontal Bar Plot
plt.bar(mixed_name, mixed_perplexity)


plt.title("Different model perplexity under Reuter's test set")
plt.xlabel('Model', fontweight ='bold', fontsize = 15)
plt.ylabel('Perplexity', fontweight ='bold', fontsize = 15)
 
# Show Plot
plt.show()

